## R - dplyr and Data Transformation

dplyr \~ 'Deployer'

A few things about data and data types in R! Like Python, we have specified types of data we'll use in our tables.

-   int is integer

-   dbl is doubles

-   chr is for character or characters

-   dttm is date-time

-   lgl is logical - similar to Booleans in Python or other programming languages; either TRUE or FALSE

-   fctr is factors (We won't use these much yet)

-   date - self explanatory.

### There are five main processes we'll use with dplyr to manipulate our data:

-   `filter()` our data by choosing data by value

-   `arrange()` data by reordering rows

-   `select()` specific data items

-   `mutate()` existing data into new data

-   `summarize()` values into a single summary

Like SQL, we'll use aggregation to break these functions down into groups; we'll do so by using a `group_by()` function as well.

dplyr functions follow a common syntax: \`function_name(\<data frame\>, \<\*args\>)\<resulting data frame\>. Many data frames in R are called 'tibbles'. Don't worry too much about what this means for now - just imagine them like tables in SQL for right now. An example of the dplyr syntax:

```{r}
# nycflights13::flights
View(flights)
# two ways to pull up 'flights' in help
?flights
help(flights)
# only get back rows which match both these values for these columns
filter(flights, month == 3, day == 15)
```

In the above example, I had two arguments - month == 3, and day == 15. This filtered down the results to only display flights that occurred on March 15, which is the day I wrote this (except the `nycflights13` data is from 2013 - RStudio was only two years old!)

Remember, too, that just like ggplot2 - we can still ask questions in our console, like `?flights` to get more information.

Much like python, we can compare values using comparison operators - `>`, `<`, `>=`, `<=`, `==`, `!=`. We also get to use & and \| for comparisons of multiple booleans.

```{r}
# just look at data from a few airlines
View(filter(flights, carrier == 'AA' | carrier == 'UA' | carrier == 'DL'))
```

### Arranging Rows

```{r}
# list desired columns left->right, then can specify sorting type & column at the end
arrange(flights, carrier, year, month, desc(sched_dep_time))
# use $ operand to access one variable in a datset, not as good as selecting columns
flights$year
```

## Selecting Columns

```{r}
View(select(flights, year, month, day))
```

(You might want to pop out the tibble into another window for better viewing).

Other useful tools for sorting through selected columns:

-   starts_with()

    ```{r}
    # applies to column names; ye'ar'
    View(select(flights, ends_with("ar")))
    ```

-   ends_with()

-   contains()

-   matches() - inside of this, you'll insert a string of Regex

-   num_range("n", 5:7) will match n5, n6, and n7

-   everything() will include all the rest of the columns. So you can put it at the end of a select statement as a parameter, and you'll be able to move selected columns to the left side, and then include the rest of the data.

### Mutating Data

Besides being a terrifying name, mutating data is an important part of exploring our data. This function allows us to combine observations from other columns to create a new column of observations. For example:

```{r}
flight_data <- select(flights,
                      year:day,
                      ends_with("delay"),
                      distance,
                      air_time
                      )
View(mutate(flight_data,
            gain = arr_delay - dep_delay,
            speed = distance / air_time * 60
            ))
```

You can now see the `gain` and `speed` columns have been created; `mutate()` always places those new columns at the end of the data set, so if you'd want to move them to the front, you'd use `select()`.

Keep in mind as you do mutations that whatever process you end up using has to be something that iterates on data sets, and that produces something iterable as well. Can't change just one thing, it goes over the whole column and returns something to the whole column.

### Summarizing Data

```{r}
# na.rm 'ignore non applicable values' so that mean works
# calculate average flight delay in 2013
View(summarize(flights, delay = mean(dep_delay, na.rm = TRUE)))
```

First off, this is kind of a weird table. It only has one piece of data in it! Not especially useful. By the way, the `na.rm = True` line asks the function to remove any missing values in the data. If we didn't have it when we use `group_by()`, we'd have a lot of observations reading `NA`, which isn't very helpful. In our current data set, the delay times for cancelled flights are populating the database with `NA` values.

Summarizing data is a lot easier if we use `group_by()` with it. This is very similar to how we use aggregate functions in SQL.

```{r}
# average delay for each day
by_day <- group_by(flights, year, month, day)
View(summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)))
```

### Using Pipe to string together functions

These functions become more and more useful when we can combine them together - often our exploration will be multi-step. Fortunately, there's a way we can combine functions together - pipe ' %\>% '.

Imagine we want to look at the relationship between distance and average delay for each location. We'll group flights by destination, figure out distance, average delay, and number of flights, and then filter out noise and the airport in Honolulu, which is so far away from other airports that it will make it hard to see the data we want to focus on.

```{r}
# this needs dplyr tidyverse libraries
# Comparison code before we use pipe:

# this is the normal, long way to find delay by destination
by_destination <- group_by(flights, dest)
delay <- summarize(by_destination,
                   count = n(),
                   # n() gives current group size
                   dist = mean(distance, na.rm = TRUE),
                   delay = mean(arr_delay, na.rm = TRUE)
                   )
delay <- filter(delay, count > 20, dest != "NHL")
View(delay)
```

A more efficient way of doing the same thing, where we don't need to write variables for each step over and over, uses pipe:

```{r}
# applies filter directly to the group_by and the group_by directly to the stored copy of the dataset
delays <- flights %>% # final destination of the pipes
  group_by(dest) %>% # apply upward
  summarize(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)
  ) %>% # apply upward 
  filter(count > 20, dest != "HNL")
view(delays)
```

This way, we can simply thread queries together without having to write new variables every time. It's inferred that each function is working on the same data frame, so we don't need to write it in as the first parameter of each function here. It's pretty normal to add a count function into our code as well, so that we know how large our data sets are - if we've queried our data down into a very small number of values, it can be easy to mistakenly draw conclusions based on a very small amount of data.

Please note: The only part of the tidyverse that won't work with pipe is ggplot2. You can pipe into it, but you can't pipe afterwards. That's because they invented dplyr after they invented ggplot2 and said 'rats that would have been cool'.

Using commas vs. pipe: commas are used to list attributes of something, while

```{r}
# Create a variable that we can re-use later that removes all cancelled flights
# '!is.na' ; as long as there are valid values for delay and arrival, then it will save those values into the filter
not_cancelled <- flights %>% # 'pipe the next line into that'
  filter(!is.na(dep_delay), !is.na(arr_delay))

not_cancelled %>%
  group_by(year, month, day) %>%
  summarize(mean = mean(dep_delay))

view(not_cancelled)
```

Now we can look at the plotted data of delay times, and try to learn something about delay times.

```{r}
delays <- not_cancelled %>%
  group_by(tailnum) %>%
  summarize(
    delay = mean(arr_delay)
  )
ggplot(data = delays, mapping = aes(x = delay)) +
  geom_freqpoly(binwidth = 10)

# most delays were 0, and some were small delays and very few were faster, then a couple existed that were very large
```

This chart makes it look like there are planes with an average delay of 5 hours, which is bizarre. Maybe we're not looking at a large enough data set! For example - maybe we're working with a small data set, like if we were only looking at a small number of total flights. Imagine if this data set were only 100 flights, and one plane had a 10 hour delay, and no delay on it's second flight. That plane would now appear to have 5 hours of delay on average! Obviously, this is a problem, and we need to be sure that we don't have too small of a data set.

```{r}
# same as before but removing NA values and measuring total occurrences of each delay value
delays <- not_cancelled %>%
  group_by(tailnum) %>%
  summarize(
    delay = mean(arr_delay, na.rm = TRUE),
    n = n() # n() = total number -> will use later as x-coord
  )

# make a see-through scatter plot with x-value being the number of times that value occurred
ggplot(data = delays, mapping = aes(x = n, y = delay)) +
  geom_point(alpha = 1/10)
```

Now we have a better idea. Let's change the plot parameters so we can get a better look at the bulk of our data - we have some outliers for flights that are greatly delayed here, and we can see that the flights with heavy delays have very few flights to their name. It's possible these planes had other issues causing their delays, and they didn't do any other flights.

```{r}
delays %>%
  # filter out delay values that occurred less than 25 times (outliers)
  filter(n > 25) %>%
  ggplot(mapping = aes(x = n, y = delay)) +
  # a dark dot means there's 10 values there
  geom_point(alpha = 1/10)
```

Looks like the majority of flights leave on time most of the time. This is a much better data set to look at to find information about the bulk of flight delays.
